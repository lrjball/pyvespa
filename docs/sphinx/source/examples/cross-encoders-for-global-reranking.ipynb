{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<picture>\n",
    "  <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://vespa.ai/assets/vespa-ai-logo-heather.svg\">\n",
    "  <source media=\"(prefers-color-scheme: light)\" srcset=\"https://vespa.ai/assets/vespa-ai-logo-rock.svg\">\n",
    "  <img alt=\"#Vespa\" width=\"200\" src=\"https://vespa.ai/assets/vespa-ai-logo-rock.svg\" style=\"margin-bottom: 25px;\">\n",
    "</picture>\n",
    "\n",
    "# Using Mixedbread.ai cross-encoder for reranking in Vespa.ai\n",
    "\n",
    "First, let us recap what cross-encoders are and where they might fit in a Vespa application.\n",
    "\n",
    "In contrast to bi-encoders, it is important to know that cross-encoders do NOT produce an embedding. Instead a cross-encoder acts on _pairs_ of input sequences and produces a single scalar score between 0 and 1 indicating the similarity or relevance between the two sentences.\n",
    "\n",
    "> The cross-encoder model is a transformer based model with a classification head on top of the Transformer CLS token (classification token).\n",
    ">\n",
    "> The model has been fine-tuned using the MS Marco passage training set and is a binary classifier which classifies if a query,document pair is relevant or not.\n",
    "\n",
    "The quote is from [this](https://blog.vespa.ai/pretrained-transformer-language-models-for-search-part-4/) blog post from 2021 that explains cross-encoders more in depth. Note that the reference to the MS Marco dataset is for the model used in the blog post, and not the model we will use in this notebook.\n",
    "\n",
    "## Properties of cross-encoders and where they fit in Vespa\n",
    "\n",
    "Cross-encoders are great at comparing a query and a document, but the time complexity increases linearly with the number of documents a query is compared to.\n",
    "\n",
    "This is why cross-encoders are often part of solutions at the top of leaderboards for ranking performance, such as MS MARCO Passage Ranking leaderboard.\n",
    "\n",
    "However, this leaderboard does not evaluate a solution's latency, and for production systems, doing cross-encoder inference for all documents in a corpus become prohibitively expensive.\n",
    "\n",
    "With Vespa's phased ranking capabilites, doing cross-encoder inference for a subset of documents at a later stage in the ranking pipeline can be a good trade-off between ranking performance and latency.\n",
    "For the remainder of this notebook, we will look at using a cross-encoder in _global-phase reranking_, introduced in [this](https://blog.vespa.ai/improving-llm-context-ranking-with-cross-encoders/) blog post.\n",
    "\n",
    "![image](https://blog.vespa.ai/assets/2023-05-08-improving-llm-context-ranking-with-cross-encoders/image1.png)\n",
    "\n",
    "In this notebook, we will show how to use the Mixedbread.ai cross-encoder for global-phase reranking in Vespa.\n",
    "\n",
    "The inference can also be run on GPU in [Vespa Cloud](https://cloud.vespa.ai/), to accelerate inference even further.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the Mixedbread.ai cross-encoder\n",
    "\n",
    "[mixedbread.ai](https://huggingface.co/mixedbread-ai) has done an amazing job of releasing both (binary) embedding-models and rerankers on huggingface ðŸ¤— the last weeks.\n",
    "\n",
    "> Check out our previous notebook on using binary embeddings from mixedbread.ai in Vespa Cloud [here](https://pyvespa.readthedocs.io/en/latest/examples/mixedbread-binary-embeddings-with-sentence-transformers-cloud.html)\n",
    "\n",
    "For this demo, we will use [mixedbread-ai/mxbai-rerank-xsmall-v1](https://huggingface.co/mixedbread-ai/mxbai-rerank-xsmall-v1), but you can experiment with the larger models, depending on how you want to balance speed, accuracy, and cost (if you want to use GPU).\n",
    "\n",
    "This model is really powerful despite its small size, and provide a good trade-off between speed and accuracy.\n",
    "\n",
    "Table of accuracy on a [BEIR](http://beir.ai) (11 datasets):\n",
    "\n",
    "| Model                      | Accuracy |\n",
    "| -------------------------- | -------- |\n",
    "| Lexical Search             | 66.4     |\n",
    "| bge-reranker-base          | 66.9     |\n",
    "| bge-reranker-large         | 70.6     |\n",
    "| cohere-embed-v3            | 70.9     |\n",
    "| **mxbai-rerank-xsmall-v1** | **70.0** |\n",
    "| mxbai-rerank-base-v1       | 72.3     |\n",
    "| mxbai-rerank-large-v1      | 74.9     |\n",
    "\n",
    "(Table from mixedbread.ai's introductory [blog post](https://www.mixedbread.ai/blog/mxbai-rerank-v1).)\n",
    "\n",
    "As we can see, the `mxbai-rerank-xsmall-v1` model is almost on par with much larger models, while being much faster and cheaper to run.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the model\n",
    "\n",
    "We will use the quantized version of `mxbai-rerank-xsmall-v1` for this demo, as it is faster and cheaper to run, but feel free to change to the model of your choice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded model to model/model_quantized.onnx\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "url = \"https://huggingface.co/mixedbread-ai/mxbai-rerank-xsmall-v1/resolve/main/onnx/model_quantized.onnx\"\n",
    "local_model_path = \"model/model_quantized.onnx\"\n",
    "\n",
    "r = requests.get(url)\n",
    "# Create path if it doesn't exist\n",
    "Path(local_model_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "with open(local_model_path, \"wb\") as f:\n",
    "    f.write(r.content)\n",
    "    print(f\"Downloaded model to {local_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting the model\n",
    "\n",
    "It is useful to inspect the expected inputs and outputs, along with their shapes, before integrating the model into Vespa.\n",
    "\n",
    "This can either be done by for instance by using the `sentence_transformers` and `onnxruntime` libraries.\n",
    "\n",
    "One-off tasks like this are well suited for a Colab notebook, one example on how to do this in colab can be found here:\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1DfubYQNyBWzBpgyGUBPN5gaIuSJZ28wy)\n",
    "\n",
    "## What does a crossencoder do?\n",
    "\n",
    "Below, we have tried to visualize what is going on in a cross-encoder, which helps us understand how we can use it in Vespa.\n",
    "\n",
    "![image](../_static/Crossencoders.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the input pairs (query, document) are prefixed with a special `[CLS]` token, and separated by a `[SEP]` token.\n",
    "\n",
    "In Vespa, we want to tokenize the document body at indexing time, and the query at query time, and then combine them in the same way as the cross-encoder does, during ranking.\n",
    "\n",
    "Let us see how we can achieve this in Vespa.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining our Vespa application\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vespa.package import (\n",
    "    Component,\n",
    "    Document,\n",
    "    Field,\n",
    "    FieldSet,\n",
    "    Function,\n",
    "    GlobalPhaseRanking,\n",
    "    OnnxModel,\n",
    "    Parameter,\n",
    "    RankProfile,\n",
    "    Schema,\n",
    ")\n",
    "\n",
    "schema = Schema(\n",
    "    name=\"doc\",\n",
    "    mode=\"index\",\n",
    "    document=Document(\n",
    "        fields=[\n",
    "            Field(name=\"id\", type=\"string\", indexing=[\"summary\", \"attribute\"]),\n",
    "            Field(\n",
    "                name=\"text\",\n",
    "                type=\"string\",\n",
    "                indexing=[\"index\", \"summary\"],\n",
    "                index=\"enable-bm25\",\n",
    "            ),\n",
    "            # LetÂ´s add a synthetic field (see https://docs.vespa.ai/en/schemas.html#field)\n",
    "            # to define how the tokens are derived from the text field\n",
    "            Field(\n",
    "                name=\"body_tokens\",\n",
    "                type=\"tensor<float>(d0[512])\",\n",
    "                # The tokenizer will be defined in the next cell\n",
    "                indexing=[\"input text\", \"embed tokenizer\", \"attribute\", \"summary\"],\n",
    "                is_document_field=False,  # Indicates a synthetic field\n",
    "            ),\n",
    "        ],\n",
    "    ),\n",
    "    fieldsets=[FieldSet(name=\"default\", fields=[\"text\"])],\n",
    "    models=[\n",
    "        OnnxModel(\n",
    "            model_name=\"crossencoder\",\n",
    "            model_file_path=f\"{local_model_path}\",\n",
    "            inputs={\n",
    "                \"input_ids\": \"input_ids\",\n",
    "                \"attention_mask\": \"attention_mask\",\n",
    "            },\n",
    "            outputs={\"logits\": \"logits\"},\n",
    "        )\n",
    "    ],\n",
    "    rank_profiles=[\n",
    "        RankProfile(name=\"bm25\", first_phase=\"bm25(text)\"),\n",
    "        RankProfile(\n",
    "            name=\"reranking\",\n",
    "            inherits=\"default\",\n",
    "            # We truncate the query to 64 tokens, meaning we have 512-64=448 tokens left for the document.\n",
    "            inputs=[(\"query(q)\", \"tensor<float>(d0[64])\")],\n",
    "            # See https://huggingface.co/mixedbread-ai/mxbai-rerank-xsmall-v1/blob/main/tokenizer_config.json\n",
    "            functions=[\n",
    "                Function(\n",
    "                    name=\"input_ids\",\n",
    "                    # See https://docs.vespa.ai/en/cross-encoders.html#roberta-based-model and https://docs.vespa.ai/en/reference/rank-features.html\n",
    "                    expression=\"customTokenInputIds(1, 2, 512, query(q), attribute(body_tokens))\",\n",
    "                ),\n",
    "                Function(\n",
    "                    name=\"attention_mask\",\n",
    "                    expression=\"tokenAttentionMask(512, query(q), attribute(body_tokens))\",\n",
    "                ),\n",
    "            ],\n",
    "            first_phase=\"bm25(text)\",\n",
    "            global_phase=GlobalPhaseRanking(\n",
    "                rerank_count=10,\n",
    "                # We use the sigmoid function to force the output to be between 0 and 1, converting logits to probabilities.\n",
    "                expression=\"sigmoid(onnx(crossencoder).logits{d0:0,d1:0})\",\n",
    "            ),\n",
    "            summary_features=[\n",
    "                \"query(q)\",\n",
    "                \"input_ids\",\n",
    "                \"attention_mask\",\n",
    "                \"onnx(crossencoder).logits\",\n",
    "            ],\n",
    "        ),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vespa.package import ApplicationPackage\n",
    "\n",
    "app_package = ApplicationPackage(\n",
    "    name=\"reranking\",\n",
    "    schema=[schema],\n",
    "    components=[\n",
    "        Component(\n",
    "            # See https://docs.vespa.ai/en/reference/embedding-reference.html#huggingface-tokenizer-embedder\n",
    "            id=\"tokenizer\",\n",
    "            type=\"hugging-face-tokenizer\",\n",
    "            parameters=[\n",
    "                Parameter(\n",
    "                    \"model\",\n",
    "                    {\n",
    "                        \"url\": \"https://huggingface.co/mixedbread-ai/mxbai-rerank-xsmall-v1/raw/main/tokenizer.json\"\n",
    "                    },\n",
    "                ),\n",
    "            ],\n",
    "        )\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is useful to inspect the schema-file (see https://docs.vespa.ai/en/reference/schema-reference.html) before deploying the application.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "schema doc {\n",
      "    document doc {\n",
      "        field id type string {\n",
      "            indexing: summary | attribute\n",
      "        }\n",
      "        field text type string {\n",
      "            indexing: index | summary\n",
      "            index: enable-bm25\n",
      "        }\n",
      "    }\n",
      "    field body_tokens type tensor<float>(d0[512]) {\n",
      "        indexing: input text | embed tokenizer | attribute | summary\n",
      "    }\n",
      "    fieldset default {\n",
      "        fields: text\n",
      "    }\n",
      "    onnx-model crossencoder {\n",
      "        file: files/crossencoder.onnx\n",
      "        input input_ids: input_ids\n",
      "        input attention_mask: attention_mask\n",
      "        output logits: logits\n",
      "    }\n",
      "    rank-profile bm25 {\n",
      "        first-phase {\n",
      "            expression {\n",
      "                bm25(text)\n",
      "            }\n",
      "        }\n",
      "    }\n",
      "    rank-profile reranking inherits default {\n",
      "        inputs {\n",
      "            query(q) tensor<float>(d0[64])             \n",
      "        \n",
      "        }\n",
      "        function input_ids() {\n",
      "            expression {\n",
      "                customTokenInputIds(1, 2, 512, query(q), attribute(body_tokens))\n",
      "            }\n",
      "        }\n",
      "        function attention_mask() {\n",
      "            expression {\n",
      "                tokenAttentionMask(512, query(q), attribute(body_tokens))\n",
      "            }\n",
      "        }\n",
      "        first-phase {\n",
      "            expression {\n",
      "                bm25(text)\n",
      "            }\n",
      "        }\n",
      "        global-phase {\n",
      "            rerank-count: 10\n",
      "            expression {\n",
      "                sigmoid(onnx(crossencoder).logits{d0:0,d1:0})\n",
      "            }\n",
      "        }\n",
      "        summary-features {\n",
      "            query(q)\n",
      "            input_ids\n",
      "            attention_mask\n",
      "            onnx(crossencoder).logits\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(schema.schema_to_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks fine. Now, let's just save the application package first, so that we also have more insight into the other files that are part of the application package.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally, we can also write the application package to disk before deploying it.\n",
    "app_package.to_files(\"crossencoder-demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for configuration server, 0/60 seconds...\n",
      "Using plain http against endpoint http://localhost:8089/ApplicationStatus\n",
      "Waiting for application status, 0/300 seconds...\n",
      "Using plain http against endpoint http://localhost:8089/ApplicationStatus\n",
      "Waiting for application status, 5/300 seconds...\n",
      "Using plain http against endpoint http://localhost:8089/ApplicationStatus\n",
      "Waiting for application status, 10/300 seconds...\n",
      "Using plain http against endpoint http://localhost:8089/ApplicationStatus\n",
      "Application is up!\n",
      "Finished deployment.\n"
     ]
    }
   ],
   "source": [
    "from vespa.deployment import VespaDocker\n",
    "\n",
    "vespa_docker = VespaDocker(port=8080)\n",
    "\n",
    "app = vespa_docker.deploy(application_package=app_package)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  1126  100  1126    0     0   5715      0 --:--:-- --:--:-- --:--:--  5686\n",
      "100  271M  100  271M    0     0  15.8M      0  0:00:17  0:00:17 --:--:-- 16.3M\n",
      "unspecified option[0](optimize model), fallback: true\n",
      "vm_size: 166648 kB, vm_rss: 46700 kB, malloc_peak: 0 kb, malloc_curr: 1100 (before loading model)\n",
      "vm_size: 517176 kB, vm_rss: 405592 kB, malloc_peak: 0 kb, malloc_curr: 351628 (after loading model)\n",
      "model meta-data:\n",
      "  input[0]: 'input_ids' long[batch_size][sequence_length]\n",
      "  input[1]: 'attention_mask' long[batch_size][sequence_length]\n",
      "  output[0]: 'logits' float[batch_size][1]\n",
      "unspecified option[1](symbolic size 'batch_size'), fallback: 1\n",
      "unspecified option[2](symbolic size 'sequence_length'), fallback: 1\n",
      "1717140328.769314\tlocalhost\t1305/26134\t-\t.eval.onnx_wrapper\twarning\tinput 'input_ids' with element type 'long' is bound to vespa value with cell type 'double'; adding explicit conversion step (this conversion might be lossy)\n",
      "1717140328.769336\tlocalhost\t1305/26134\t-\t.eval.onnx_wrapper\twarning\tinput 'attention_mask' with element type 'long' is bound to vespa value with cell type 'double'; adding explicit conversion step (this conversion might be lossy)\n",
      "test setup:\n",
      "  input[0]: tensor(d0[1],d1[1]) -> long[1][1]\n",
      "  input[1]: tensor(d0[1],d1[1]) -> long[1][1]\n",
      "  output[0]: float[1][1] -> tensor<float>(d0[1],d1[1])\n",
      "unspecified option[3](max concurrent evaluations), fallback: 1\n",
      "vm_size: 517176 kB, vm_rss: 405592 kB, malloc_peak: 0 kb, malloc_curr: 351628 (no evaluations yet)\n",
      "vm_size: 517176 kB, vm_rss: 405856 kB, malloc_peak: 0 kb, malloc_curr: 351628 (concurrent evaluations: 1)\n",
      "estimated model evaluation time: 3.77819 ms\n"
     ]
    }
   ],
   "source": [
    "from docker.models.containers import Container\n",
    "\n",
    "\n",
    "def download_and_analyze_model(url: str, container: Container) -> None:\n",
    "    \"\"\"\n",
    "    Downloads an ONNX model from a specified URL and analyzes it within a Docker container.\n",
    "\n",
    "    Parameters:\n",
    "    url (str): The URL from where the ONNX model should be downloaded.\n",
    "    container (Container): The Docker container in which the command will be executed.\n",
    "\n",
    "    Raises:\n",
    "    Exception: Raises an exception if the command execution fails or if there are issues in streaming the output.\n",
    "\n",
    "    Note:\n",
    "    This function assumes that 'curl' and 'vespa-analyze-onnx-model' are available in the container environment.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the path inside the container where the model will be stored.\n",
    "    model_path = \"/opt/vespa/var/model.onnx\"\n",
    "\n",
    "    # Construct the command to download and analyze the model inside the container.\n",
    "    command = f\"bash -c 'curl -Lo {model_path} {url} && vespa-analyze-onnx-model {model_path}'\"\n",
    "\n",
    "    # Command to delete the model after analysis.\n",
    "    delete_command = f\"rm {model_path}\"\n",
    "\n",
    "    # Execute the command in the container and handle potential errors.\n",
    "    try:\n",
    "        exit_code, output = container.exec_run(command, stream=True)\n",
    "        # Print the output from the command.\n",
    "        for line in output:\n",
    "            print(line.decode(), end=\"\")\n",
    "        # Remove the model after analysis.\n",
    "        container.exec_run(delete_command)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "url = \"https://huggingface.co/mixedbread-ai/mxbai-rerank-xsmall-v1/resolve/main/onnx/model.onnx\"\n",
    "# Example usage:\n",
    "# download_and_analyze_model(url, vespa_docker.container)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By doing this with the different size models and their quantized versions, we get this table.\n",
    "\n",
    "| Model                                | Model File           | Inference Time (ms) | Size   | N docs in 200ms |\n",
    "| ------------------------------------ | -------------------- | ------------------- | ------ | --------------- |\n",
    "| mixedbread-ai/mxbai-rerank-xsmall-v1 | model_quantized.onnx | 2.4                 | 87MB   | 82              |\n",
    "| mixedbread-ai/mxbai-rerank-xsmall-v1 | model.onnx           | 3.8                 | 284MB  | 52              |\n",
    "| mixedbread-ai/mxbai-rerank-base-v1   | model_quantized.onnx | 5.4                 | 244MB  | 37              |\n",
    "| mixedbread-ai/mxbai-rerank-base-v1   | model.onnx           | 10.3                | 739MB  | 19              |\n",
    "| mixedbread-ai/mxbai-rerank-large-v1  | model_quantized.onnx | 16.0                | 643MB  | 12              |\n",
    "| mixedbread-ai/mxbai-rerank-large-v1  | model.onnx           | 35.6                | 1.74GB | 5               |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a time budget of 200ms for reranking, we can add a column indicating the number of documents we are able to rerank within the budget time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed a few sample documents to the application\n",
    "sample_docs = [\n",
    "    {\"id\": i, \"fields\": {\"text\": text}}\n",
    "    for i, text in enumerate(\n",
    "        [\n",
    "            \"'To Kill a Mockingbird' is a novel by Harper Lee published in 1960. It was immediately successful, winning the Pulitzer Prize, and has become a classic of modern American literature.\",\n",
    "            \"The novel 'Moby-Dick' was written by Herman Melville and first published in 1851. It is considered a masterpiece of American literature and deals with complex themes of obsession, revenge, and the conflict between good and evil.\",\n",
    "            \"Harper Lee, an American novelist widely known for her novel 'To Kill a Mockingbird', was born in 1926 in Monroeville, Alabama. She received the Pulitzer Prize for Fiction in 1961.\",\n",
    "            \"Jane Austen was an English novelist known primarily for her six major novels, which interpret, critique and comment upon the British landed gentry at the end of the 18th century.\",\n",
    "            \"The 'Harry Potter' series, which consists of seven fantasy novels written by British author J.K. Rowling, is among the most popular and critically acclaimed books of the modern era.\",\n",
    "            \"'The Great Gatsby', a novel written by American author F. Scott Fitzgerald, was published in 1925. The story is set in the Jazz Age and follows the life of millionaire Jay Gatsby and his pursuit of Daisy Buchanan.\",\n",
    "        ]\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vespa.io import VespaResponse\n",
    "\n",
    "\n",
    "def callback(response: VespaResponse, id: str):\n",
    "    if not response.is_successful():\n",
    "        print(\n",
    "            f\"Failed to feed document {id} with status code {response.status_code}: Reason {response.get_json()}\"\n",
    "        )\n",
    "\n",
    "\n",
    "app.feed_iterable(sample_docs, schema=\"doc\", callback=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"'To Kill a Mockingbird' is a novel by Harper Lee published in 1960. It was \"\n",
      " 'immediately successful, winning the Pulitzer Prize, and has become a classic '\n",
      " 'of modern American literature.')\n",
      "0.9634037778787636\n",
      "(\"Harper Lee, an American novelist widely known for her novel 'To Kill a \"\n",
      " \"Mockingbird', was born in 1926 in Monroeville, Alabama. She received the \"\n",
      " 'Pulitzer Prize for Fiction in 1961.')\n",
      "0.8672221280618897\n",
      "(\"'The Great Gatsby', a novel written by American author F. Scott Fitzgerald, \"\n",
      " 'was published in 1925. The story is set in the Jazz Age and follows the life '\n",
      " 'of millionaire Jay Gatsby and his pursuit of Daisy Buchanan.')\n",
      "0.09325768904619067\n",
      "(\"The novel 'Moby-Dick' was written by Herman Melville and first published in \"\n",
      " '1851. It is considered a masterpiece of American literature and deals with '\n",
      " 'complex themes of obsession, revenge, and the conflict between good and '\n",
      " 'evil.')\n",
      "0.010269765303083865\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "with app.syncio(connections=1) as sync_app:\n",
    "    query = sync_app.query(\n",
    "        body={\n",
    "            \"yql\": \"select * from sources * where userQuery();\",\n",
    "            \"query\": \"who wrote to kill a mockingbird?\",\n",
    "            \"input.query(q)\": \"embed(tokenizer, @query)\",\n",
    "            \"ranking.profile\": \"reranking\",\n",
    "            \"ranking.listFeatures\": \"true\",\n",
    "            \"presentation.timing\": \"true\",\n",
    "        }\n",
    "    )\n",
    "    for hit in query.hits:\n",
    "        pprint(hit[\"fields\"][\"text\"])\n",
    "        pprint(hit[\"relevance\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will of course be necessary to evaluate the performance of the cross-encoder in your specific use-case, but this notebook should give you a good starting point.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "Try to use [hybrid search](https://pyvespa.readthedocs.io/en/latest/getting-started-pyvespa-cloud.html) for the first phase, and rerank with a cross-encoder.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vespa_docker.container.stop()\n",
    "vespa_docker.container.remove()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyvespa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
